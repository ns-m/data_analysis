{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ns-m/data_analysis/blob/main/Copy_of_%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22adanet_objective%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9QvCK6Su4hY"
      },
      "source": [
        "##### Copyright 2018 The AdaNet Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUman0Uju6SR"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5CQ48UOxrqO"
      },
      "source": [
        "# The AdaNet objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tw35taebIOz"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/adanet/blob/master/adanet/examples/tutorials/adanet_objective.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/adanet_objective.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kL896ITmpoR"
      },
      "source": [
        "One of key contributions from *AdaNet: Adaptive Structural Learning of Neural\n",
        "Networks* [[Cortes et al., ICML 2017](https://arxiv.org/abs/1607.01097)] is\n",
        "defining an algorithm that aims to directly minimize the DeepBoost\n",
        "generalization bound from *Deep Boosting*\n",
        "[[Cortes et al., ICML 2014](http://proceedings.mlr.press/v32/cortesb14.pdf)]\n",
        "when applied to neural networks. This algorithm, called **AdaNet**, adaptively\n",
        "grows a neural network as an ensemble of subnetworks that minimizes the AdaNet\n",
        "objective (a.k.a. AdaNet loss):\n",
        "\n",
        "$$F(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\Phi \\left(\\sum_{j=1}^{N}w_jh_j(x_i), y_i \\right) + \\sum_{j=1}^{N} \\left(\\lambda r(h_j) + \\beta \\right) |w_j| $$\n",
        "\n",
        "where $w$ is the set of mixture weights, one per subnetwork $h$,\n",
        "$\\Phi$ is a surrogate loss function such as logistic loss or MSE, $r$ is a\n",
        "function for measuring a subnetwork's complexity, and $\\lambda$ and $\\beta$\n",
        "are hyperparameters.\n",
        "\n",
        "## Mixture weights\n",
        "\n",
        "So what are mixture weights? When forming an ensemble $f$ of subnetworks $h$,\n",
        "we need to somehow combine their predictions. This is done by multiplying\n",
        "the outputs of subnetwork $h_i$ with mixture weight $w_i$, and summing the\n",
        "results:\n",
        "\n",
        "$$f(x) = \\sum_{j=1}^{N}w_jh_j(x)$$\n",
        "\n",
        "In practice, most commonly used set of mixture weight is **uniform average\n",
        "weighting**:\n",
        "\n",
        "$$f(x) = \\frac{1}{N}\\sum_{j=1}^{N}h_j(x)$$\n",
        "\n",
        "However, we can also solve a convex optimization problem to learn the mixture\n",
        "weights that minimize the loss function $\\Phi$:\n",
        "\n",
        "$$F(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\Phi \\left(\\sum_{j=1}^{N}w_jh_j(x_i), y_i \\right)$$\n",
        "\n",
        "This is the first term in the AdaNet objective. The second term applies L1\n",
        "regularization to the mixture weights:\n",
        "\n",
        "$$\\sum_{j=1}^{N} \\left(\\lambda r(h_j) + \\beta \\right) |w_j|$$\n",
        "\n",
        "When $\\lambda > 0$ this penalty serves to prevent the optimization from\n",
        "assigning too much weight to more complex subnetworks according to the\n",
        "complexity measure function $r$.\n",
        "\n",
        "## How AdaNet uses the objective\n",
        "\n",
        "This objective function serves two purposes:\n",
        "\n",
        "1.  To **learn to scale/transform the outputs of each subnetwork $h$** as part\n",
        "    of the ensemble.\n",
        "2.  To **select the best candidate subnetwork $h$** at each AdaNet iteration\n",
        "    to include in the ensemble.\n",
        "\n",
        "Effectively, when learning mixture weights $w$, AdaNet solves a convex\n",
        "combination of the outputs of the frozen subnetworks $h$. For $\\lambda >0$,\n",
        "AdaNet penalizes more complex subnetworks with greater L1 regularization on\n",
        "their mixture weight, and will be less likely to select more complex subnetworks\n",
        "to add to the ensemble at each iteration.\n",
        "\n",
        "In this tutorial, you will observe the benefits of using AdaNet to learn the\n",
        "ensemble's mixture weights and to perform candidate selection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWSc6JsqAGjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a860f4-408e-46ac-fbef-ee2e81ddc1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adanet\n",
            "  Downloading adanet-0.9.0-py2.py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2.0,>=1.11 in /usr/local/lib/python3.7/dist-packages (from adanet) (1.15.0)\n",
            "Collecting nose<2.0,>=1.3\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 44.5 MB/s \n",
            "\u001b[?25hCollecting coverage<5.0,>=4.5\n",
            "  Downloading coverage-4.5.4-cp37-cp37m-manylinux1_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.7/dist-packages (from adanet) (1.21.6)\n",
            "Collecting mock<4.0,>=3.0\n",
            "  Downloading mock-3.0.5-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.6 in /usr/local/lib/python3.7/dist-packages (from adanet) (3.17.3)\n",
            "Collecting rednose<2.0,>=1.3\n",
            "  Downloading rednose-1.3.0.tar.gz (10 kB)\n",
            "Collecting absl-py<1.0,>=0.7\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rednose<2.0,>=1.3->adanet) (57.4.0)\n",
            "Collecting termstyle>=0.1.7\n",
            "  Downloading termstyle-0.1.11.tar.gz (4.6 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: rednose, termstyle\n",
            "  Building wheel for rednose (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rednose: filename=rednose-1.3.0-py3-none-any.whl size=12154 sha256=54fa450967d0a94c72d1b7ce741dfcef5aee9d00d0e59480a4346a1b51e70fa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/1b/85/bdcb6d337866eb3ee39dc5273db143becd395fe9b36be75ad8\n",
            "  Building wheel for termstyle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termstyle: filename=termstyle-0.1.11-py3-none-any.whl size=4779 sha256=32df348d319d41b8487990c67d85dd9c7562296b0b46bd0d59ec2717fc6dc471\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/d6/76/3b8d5b20a6c74b92767cb90ce74734e014e78d0973c3d67d3c\n",
            "Successfully built rednose termstyle\n",
            "Installing collected packages: termstyle, colorama, rednose, nose, mock, coverage, absl-py, adanet\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.3.0\n",
            "    Uninstalling absl-py-1.3.0:\n",
            "      Successfully uninstalled absl-py-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 adanet-0.9.0 colorama-0.4.6 coverage-4.5.4 mock-3.0.5 nose-1.3.7 rednose-1.3.0 termstyle-0.1.11\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "# If you're running this in Colab, first install the adanet package:\n",
        "!pip install adanet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4ptB-vwWEGH",
        "outputId": "9e4acc5d-8505-43fc-de13-1eaafdbfd7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/adanet/core/tpu_estimator.py:33: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import adanet\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# The random seed to use.\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "LOG_DIR = '/tmp/models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElcMSpyg_dYO"
      },
      "source": [
        "## Boston Housing dataset\n",
        "\n",
        "In this example, we will solve a regression task known as the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) to predict the price of suburban houses in Boston, MA in the 1970s. There are 13 numerical features, the labels are in thousands of dollars, and there are only 506 examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp5TTBOJ_ZTU"
      },
      "source": [
        "## Download the data\n",
        "Conveniently, the data is available via Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plx4LtD4_XFY",
        "outputId": "728242f2-7fc8-4ccd-e6b7-f001e1ae6650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n",
            "Model inputs: [  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
            "   3.9769    4.      307.       21.      396.9      18.72   ] \n",
            "\n",
            "Model output (house price): $15200.0 \n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = (\n",
        "    tf.keras.datasets.boston_housing.load_data())\n",
        "\n",
        "# Preview the first example from the training data\n",
        "print('Model inputs: %s \\n' % x_train[0])\n",
        "print('Model output (house price): $%s ' % (y_train[0] * 1000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUSb0JtOA5Xc"
      },
      "source": [
        "## Supply the data in TensorFlow\n",
        "\n",
        "Our first task is to supply the data in TensorFlow. Using the\n",
        "tf.estimator.Estimator convention, we will define a function that returns an\n",
        "input_fn which returns feature and label Tensors.\n",
        "\n",
        "We will also use the tf.data.Dataset API to feed the data into our models.\n",
        "\n",
        "Also, as a preprocessing step, we will apply `tf.log1p` to log-scale the\n",
        "features and labels for improved numerical stability during training. To recover\n",
        "the model's predictions in the correct scale, you can apply `tf.math.expm1` to the\n",
        "prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1U6DppVwTPM"
      },
      "outputs": [],
      "source": [
        "FEATURES_KEY = \"x\"\n",
        "\n",
        "\n",
        "def input_fn(partition, training, batch_size):\n",
        "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
        "\n",
        "  def _input_fn():\n",
        "\n",
        "    if partition == \"train\":\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "          FEATURES_KEY: tf.log1p(x_train)\n",
        "      }, tf.log1p(y_train)))\n",
        "    else:\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "          FEATURES_KEY: tf.log1p(x_test)\n",
        "      }, tf.log1p(y_test)))\n",
        "\n",
        "    # We call repeat after shuffling, rather than before, to prevent separate\n",
        "    # epochs from blending together.\n",
        "    if training:\n",
        "      dataset = dataset.shuffle(10 * batch_size, seed=RANDOM_SEED).repeat()\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    features, labels = iterator.get_next()\n",
        "    return features, labels\n",
        "\n",
        "  return _input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXy9pqlq_PQx"
      },
      "source": [
        "## Define the subnetwork generator\n",
        "\n",
        "Let's define a subnetwork generator similar to the one in\n",
        "[[Cortes et al., ICML 2017](https://arxiv.org/abs/1607.01097)] and in\n",
        "`simple_dnn.py` which creates two candidate fully-connected neural networks at\n",
        "each iteration with the same width, but one an additional hidden layer. To make\n",
        "our generator *adaptive*, each subnetwork will have at least the same number\n",
        "of hidden layers as the most recently added subnetwork to the\n",
        "`previous_ensemble`.\n",
        "\n",
        "We define the complexity measure function $r$ to be $r(h) = \\sqrt{d(h)}$, where\n",
        "$d$ is the number of hidden layers in the neural network $h$, to approximate the\n",
        "Rademacher bounds from\n",
        "[[Golowich et. al, 2017](https://arxiv.org/abs/1712.06541)]. So subnetworks\n",
        "with more hidden layers, and therefore more capacity, will have more heavily\n",
        "regularized mixture weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZn9VeFUHvD-"
      },
      "outputs": [],
      "source": [
        "_NUM_LAYERS_KEY = \"num_layers\"\n",
        "\n",
        "\n",
        "class _SimpleDNNBuilder(adanet.subnetwork.Builder):\n",
        "  \"\"\"Builds a DNN subnetwork for AdaNet.\"\"\"\n",
        "\n",
        "  def __init__(self, optimizer, layer_size, num_layers, learn_mixture_weights,\n",
        "               seed):\n",
        "    \"\"\"Initializes a `_DNNBuilder`.\n",
        "\n",
        "    Args:\n",
        "      optimizer: An `Optimizer` instance for training both the subnetwork and\n",
        "        the mixture weights.\n",
        "      layer_size: The number of nodes to output at each hidden layer.\n",
        "      num_layers: The number of hidden layers.\n",
        "      learn_mixture_weights: Whether to solve a learning problem to find the\n",
        "        best mixture weights, or use their default value according to the\n",
        "        mixture weight type. When `False`, the subnetworks will return a no_op\n",
        "        for the mixture weight train op.\n",
        "      seed: A random seed.\n",
        "\n",
        "    Returns:\n",
        "      An instance of `_SimpleDNNBuilder`.\n",
        "    \"\"\"\n",
        "\n",
        "    self._optimizer = optimizer\n",
        "    self._layer_size = layer_size\n",
        "    self._num_layers = num_layers\n",
        "    self._learn_mixture_weights = learn_mixture_weights\n",
        "    self._seed = seed\n",
        "\n",
        "  def build_subnetwork(self,\n",
        "                       features,\n",
        "                       logits_dimension,\n",
        "                       training,\n",
        "                       iteration_step,\n",
        "                       summary,\n",
        "                       previous_ensemble=None):\n",
        "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
        "\n",
        "    input_layer = tf.to_float(features[FEATURES_KEY])\n",
        "    kernel_initializer = tf.glorot_uniform_initializer(seed=self._seed)\n",
        "    last_layer = input_layer\n",
        "    for _ in range(self._num_layers):\n",
        "      last_layer = tf.layers.dense(\n",
        "          last_layer,\n",
        "          units=self._layer_size,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_initializer=kernel_initializer)\n",
        "    logits = tf.layers.dense(\n",
        "        last_layer,\n",
        "        units=logits_dimension,\n",
        "        kernel_initializer=kernel_initializer)\n",
        "\n",
        "    shared = {_NUM_LAYERS_KEY: self._num_layers}\n",
        "    return adanet.Subnetwork(\n",
        "        last_layer=last_layer,\n",
        "        logits=logits,\n",
        "        complexity=self._measure_complexity(),\n",
        "        shared=shared)\n",
        "\n",
        "  def _measure_complexity(self):\n",
        "    \"\"\"Approximates Rademacher complexity as the square-root of the depth.\"\"\"\n",
        "    return tf.sqrt(tf.to_float(self._num_layers))\n",
        "\n",
        "  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n",
        "                                iteration_step, summary, previous_ensemble):\n",
        "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
        "    return self._optimizer.minimize(loss=loss, var_list=var_list)\n",
        "\n",
        "  @property\n",
        "  def name(self):\n",
        "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
        "\n",
        "    if self._num_layers == 0:\n",
        "      # A DNN with no hidden layers is a linear model.\n",
        "      return \"linear\"\n",
        "    return \"{}_layer_dnn\".format(self._num_layers)\n",
        "\n",
        "\n",
        "class SimpleDNNGenerator(adanet.subnetwork.Generator):\n",
        "  \"\"\"Generates a two DNN subnetworks at each iteration.\n",
        "\n",
        "  The first DNN has an identical shape to the most recently added subnetwork\n",
        "  in `previous_ensemble`. The second has the same shape plus one more dense\n",
        "  layer on top. This is similar to the adaptive network presented in Figure 2 of\n",
        "  [Cortes et al. ICML 2017](https://arxiv.org/abs/1607.01097), without the\n",
        "  connections to hidden layers of networks from previous iterations.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               optimizer,\n",
        "               layer_size=64,\n",
        "               learn_mixture_weights=False,\n",
        "               seed=None):\n",
        "    \"\"\"Initializes a DNN `Generator`.\n",
        "\n",
        "    Args:\n",
        "      optimizer: An `Optimizer` instance for training both the subnetwork and\n",
        "        the mixture weights.\n",
        "      layer_size: Number of nodes in each hidden layer of the subnetwork\n",
        "        candidates. Note that this parameter is ignored in a DNN with no hidden\n",
        "        layers.\n",
        "      learn_mixture_weights: Whether to solve a learning problem to find the\n",
        "        best mixture weights, or use their default value according to the\n",
        "        mixture weight type. When `False`, the subnetworks will return a no_op\n",
        "        for the mixture weight train op.\n",
        "      seed: A random seed.\n",
        "\n",
        "    Returns:\n",
        "      An instance of `Generator`.\n",
        "    \"\"\"\n",
        "\n",
        "    self._seed = seed\n",
        "    self._dnn_builder_fn = functools.partial(\n",
        "        _SimpleDNNBuilder,\n",
        "        optimizer=optimizer,\n",
        "        layer_size=layer_size,\n",
        "        learn_mixture_weights=learn_mixture_weights)\n",
        "\n",
        "  def generate_candidates(self, previous_ensemble, iteration_number,\n",
        "                          previous_ensemble_reports, all_reports):\n",
        "    \"\"\"See `adanet.subnetwork.Generator`.\"\"\"\n",
        "\n",
        "    num_layers = 0\n",
        "    seed = self._seed\n",
        "    if previous_ensemble:\n",
        "      num_layers = previous_ensemble.subnetworks[-1].shared[_NUM_LAYERS_KEY]\n",
        "    if seed is not None:\n",
        "      seed += iteration_number\n",
        "    return [\n",
        "        self._dnn_builder_fn(num_layers=num_layers, seed=seed),\n",
        "        self._dnn_builder_fn(num_layers=num_layers + 1, seed=seed),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhu2zpf9faIB"
      },
      "source": [
        "## Launch TensorBoard\n",
        "\n",
        "Let's run [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) to visualize model training over time. We'll use [ngrok](https://ngrok.com/) to tunnel traffic to localhost.\n",
        "\n",
        "*The instructions for setting up Tensorboard were obtained from https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/*\n",
        "\n",
        "Run the next cells and follow the link to see the TensorBoard in a new tab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLo_xWQMe686",
        "outputId": "be7dd17a-c3d2-41dc-b494-de4f00ad249c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-03 08:48:46--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  34.7MB/s    in 0.4s    \n",
            "\n",
            "2022-11-03 08:48:47 (34.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Follow this link to open TensorBoard in a new tab.\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "# Install ngrok binary.\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Delete old logs dir.\n",
        "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
        "\n",
        "print(\"Follow this link to open TensorBoard in a new tab.\")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJRH-v5NhnY9"
      },
      "source": [
        "## Train and evaluate\n",
        "\n",
        "Next we create an `adanet.Estimator` using the `SimpleDNNGenerator` we just defined.\n",
        "\n",
        "In this section we will show the effects of two hyperparamters: **learning mixture weights** and **complexity regularization**.\n",
        "\n",
        "On the righthand side you will be able to play with the hyperparameters of this model. Until you reach the end of this section, we ask that you not change them.\n",
        "\n",
        "At first we will not learn the mixture weights, using their default initial value. Here they will be scalars initialized to $1/N$ where $N$ is the number of subnetworks in the ensemble, effectively creating a **uniform average ensemble**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TJHr1k-zYpN"
      },
      "outputs": [],
      "source": [
        "#@title AdaNet parameters\n",
        "LEARNING_RATE = 0.001  #@param {type:\"number\"}\n",
        "TRAIN_STEPS = 60000  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "\n",
        "LEARN_MIXTURE_WEIGHTS = False  #@param {type:\"boolean\"}\n",
        "ADANET_LAMBDA = 0  #@param {type:\"number\"}\n",
        "ADANET_ITERATIONS = 3  #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "def train_and_evaluate(experiment_name, learn_mixture_weights=LEARN_MIXTURE_WEIGHTS,\n",
        "                       adanet_lambda=ADANET_LAMBDA):\n",
        "  \"\"\"Trains an `adanet.Estimator` to predict housing prices.\"\"\"\n",
        "\n",
        "  model_dir = os.path.join(LOG_DIR, experiment_name)\n",
        "\n",
        "  ensembler_optimizer = None\n",
        "  if learn_mixture_weights:\n",
        "    ensembler_optimizer = tf.train.RMSPropOptimizer(\n",
        "        learning_rate=LEARNING_RATE)\n",
        "\n",
        "  estimator = adanet.Estimator(\n",
        "      # Since we are predicting housing prices, we'll use a regression\n",
        "      # head that optimizes for MSE.\n",
        "      head=tf.estimator.RegressionHead(),\n",
        "\n",
        "      # Define the generator, which defines our search space of subnetworks\n",
        "      # to train as candidates to add to the final AdaNet model.\n",
        "      subnetwork_generator=SimpleDNNGenerator(\n",
        "          optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n",
        "          learn_mixture_weights=learn_mixture_weights,\n",
        "          seed=RANDOM_SEED),\n",
        "\n",
        "      # The number of train steps per iteration.\n",
        "      max_iteration_steps=TRAIN_STEPS // ADANET_ITERATIONS,\n",
        "\n",
        "      # The evaluator will evaluate the model on the full training set to\n",
        "      # compute the overall AdaNet loss (train loss + complexity\n",
        "      # regularization) to select the best candidate to include in the\n",
        "      # final AdaNet model.\n",
        "      evaluator=adanet.Evaluator(\n",
        "          input_fn=input_fn(\"train\", training=False, batch_size=BATCH_SIZE)),\n",
        "\n",
        "      ensemblers=[\n",
        "          adanet.ensemble.ComplexityRegularizedEnsembler(\n",
        "              optimizer=ensembler_optimizer,\n",
        "              # Lambda is a the strength of complexity regularization. A larger\n",
        "              # value will penalize more complex subnetworks.\n",
        "              adanet_lambda=adanet_lambda),\n",
        "      ],\n",
        "\n",
        "      # Configuration for Estimators.\n",
        "      config=tf.estimator.RunConfig(\n",
        "          save_summary_steps=5000,\n",
        "          save_checkpoints_steps=5000,\n",
        "          tf_random_seed=RANDOM_SEED,\n",
        "          model_dir=model_dir))\n",
        "\n",
        "  # Train and evaluate using using the tf.estimator tooling.\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      input_fn=input_fn(\"train\", training=True, batch_size=BATCH_SIZE),\n",
        "      max_steps=TRAIN_STEPS)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      input_fn=input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n",
        "      steps=None,\n",
        "      start_delay_secs=1,\n",
        "      throttle_secs=30,\n",
        "  )\n",
        "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "  return estimator.evaluate(\n",
        "      input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n",
        "      steps=None)\n",
        "\n",
        "def ensemble_architecture(result):\n",
        "  \"\"\"Extracts the ensemble architecture from evaluation results.\"\"\"\n",
        "\n",
        "  architecture = result[\"architecture/adanet/ensembles\"]\n",
        "  # The architecture is a serialized Summary proto for TensorBoard.\n",
        "  summary_proto = tf.summary.Summary.FromString(architecture)\n",
        "  return summary_proto.value[0].tensor.string_val[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U27fd6Kwhk3r",
        "outputId": "a94f2f8c-2e11-4d8d-b7d0-2618e661141f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fca0d568cb0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From <ipython-input-4-48abf98e4055>:24: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/adanet/core/estimator.py:1480: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc99d19e8c0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc993493f80>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc99598d0e0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1175: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1068: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc9956e25f0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc99237ddd0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc9939bb830>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc994683ef0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc996032320>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc996032320>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc9922f9200>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc98aa2f320>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc99589fcb0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7fc99629f170>) includes params argument, but params are not passed to Estimator.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.038238864\n",
            "Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"
          ]
        }
      ],
      "source": [
        "results = train_and_evaluate(\"uniform_average_ensemble_baseline\")\n",
        "print(\"Loss:\", results[\"average_loss\"])\n",
        "print(\"Architecture:\", ensemble_architecture(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0vBGKeTj0Kf"
      },
      "source": [
        "These hyperparameters preduce a model that achieves **0.0351** MSE on the test\n",
        "set (exact MSE will vary depending on the hardware you're using to train the model). Notice that the ensemble is composed of 3 subnetworks, each one a hidden\n",
        "layer deeper than the previous. The most complex subnetwork is made of 3 hidden\n",
        "layers.\n",
        "\n",
        "Since `SimpleDNNGenerator` produces subnetworks of varying complexity, and our\n",
        "model gives each one an equal weight, AdaNet selected the subnetwork that most\n",
        "lowered the ensemble's training loss at each iteration, likely the one with the\n",
        "most hidden layers, since it has the most capacity, and we aren't penalizing\n",
        "more complex subnetworks (yet).\n",
        "\n",
        "Next, instead of assigning equal weight to each subnetwork, let's learn the\n",
        "mixture weights as a convex optimization problem using SGD:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXfJ6cb_KO35",
        "outputId": "31ee2b2a-b0a1-4c80-a6b7-f7f931c66f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5deb07a0>) includes params argument, but params are not passed to Estimator.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5abe6830>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f536cd830>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5af19560>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5fb84e60>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5dd533b0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f630917a0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5f216f80>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5f1b1e60>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5dfb8170>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5e3699e0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5b02f680>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5e807170>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53330050>) includes params argument, but params are not passed to Estimator.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.039372098\n",
            "Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "results = train_and_evaluate(\"learn_mixture_weights\", learn_mixture_weights=True)\n",
        "print(\"Loss:\", results[\"average_loss\"])\n",
        "print(\"Architecture:\", ensemble_architecture(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTUdJ_FWl84O"
      },
      "source": [
        "Learning the mixture weights produces a model with **0.0343** MSE, a bit better\n",
        "than the uniform average model, which the `adanet.Estimator` always compute as a\n",
        "baseline. The mixture weights were learned without regularization, so they\n",
        "risk overfitting the training set.\n",
        "\n",
        "Observe that AdaNet learned the same ensemble composition as the previous run.\n",
        "Without complexity regularization, AdaNet will favor more complex subnetworks,\n",
        "which may have worse generalization despite improving the empirical error.\n",
        "\n",
        "Finally, let's apply some **complexity regularization** by using $\\lambda > 0$.\n",
        "Since this will penalize more complex subnetworks, AdaNet will select the\n",
        "candidate subnetwork that most improves the objective for its marginal\n",
        "complexity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g_C-dT7KaTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cb8581-f292-4126-8de3-4397a9922fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f530feef0>) includes params argument, but params are not passed to Estimator.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5e202200>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53eeb950>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5307b0e0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5b07c320>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53f4aa70>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5f112290>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53857170>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5c8164d0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f626b4d40>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5dfa5b00>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5d38bb00>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5792cc20>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
            "WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5404e320>) includes params argument, but params are not passed to Estimator.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.038760666\n",
            "Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "results = train_and_evaluate(\"learn_mixture_weights_with_complexity_regularization\", learn_mixture_weights=True, adanet_lambda=.001)\n",
        "print(\"Loss:\", results[\"average_loss\"])\n",
        "print(\"Architecture:\", ensemble_architecture(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5RuM582oxo_"
      },
      "source": [
        "Learning the mixture weights with $\\lambda > 0$ produces a model with **0.0340**\n",
        "MSE.\n",
        "\n",
        "Inspecting the ensemble architecture demonstrates the effects of complexity\n",
        "regularization on candidate selection. The selected subnetworks are relatively\n",
        "less complex: unlike in previous runs, the deepest subnetwork has only 2 hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajRhOvHGQu6w"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, you were able to explore training an AdaNet model's mixture\n",
        "weights with $\\lambda \\ge 0$. You were also able to compare against building an\n",
        "ensemble formed by always choosing the best candidate subnetwork at each\n",
        "iteration based on it's ability to improve the ensemble's loss on the training\n",
        "set, and averaging their results.\n",
        "\n",
        "Uniform average ensembles work unreasonably well in practice, yet learning the\n",
        "mixture weights with the correct values of $\\lambda$ and $\\beta$ should always\n",
        "produce a better model when candidates have varying complexity. However, this\n",
        "does require some additional hyperparameter tuning, so practically you can train\n",
        "an AdaNet with the default mixture weights and $\\lambda=0$ first, and once you\n",
        "have confirmed that the subnetworks are training correctly, you can tune the\n",
        "mixture weight hyperparameters.\n",
        "\n",
        "While this example explored a regression task, these observations apply to using\n",
        "AdaNet on other tasks like binary-classification and multi-class classification."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}