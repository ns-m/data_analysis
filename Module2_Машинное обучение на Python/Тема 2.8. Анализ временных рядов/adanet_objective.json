{"cells":[{"cell_type":"markdown","metadata":{"id":"g9QvCK6Su4hY"},"source":["##### Copyright 2018 The AdaNet Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUman0Uju6SR"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"l5CQ48UOxrqO"},"source":["# The AdaNet objective"]},{"cell_type":"markdown","metadata":{"id":"3tw35taebIOz"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/adanet/blob/master/adanet/examples/tutorials/adanet_objective.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/adanet_objective.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"7kL896ITmpoR"},"source":["One of key contributions from *AdaNet: Adaptive Structural Learning of Neural\n","Networks* [[Cortes et al., ICML 2017](https://arxiv.org/abs/1607.01097)] is\n","defining an algorithm that aims to directly minimize the DeepBoost\n","generalization bound from *Deep Boosting*\n","[[Cortes et al., ICML 2014](http://proceedings.mlr.press/v32/cortesb14.pdf)]\n","when applied to neural networks. This algorithm, called **AdaNet**, adaptively\n","grows a neural network as an ensemble of subnetworks that minimizes the AdaNet\n","objective (a.k.a. AdaNet loss):\n","\n","$$F(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\Phi \\left(\\sum_{j=1}^{N}w_jh_j(x_i), y_i \\right) + \\sum_{j=1}^{N} \\left(\\lambda r(h_j) + \\beta \\right) |w_j| $$\n","\n","where $w$ is the set of mixture weights, one per subnetwork $h$,\n","$\\Phi$ is a surrogate loss function such as logistic loss or MSE, $r$ is a\n","function for measuring a subnetwork's complexity, and $\\lambda$ and $\\beta$\n","are hyperparameters.\n","\n","## Mixture weights\n","\n","So what are mixture weights? When forming an ensemble $f$ of subnetworks $h$,\n","we need to somehow combine their predictions. This is done by multiplying\n","the outputs of subnetwork $h_i$ with mixture weight $w_i$, and summing the\n","results:\n","\n","$$f(x) = \\sum_{j=1}^{N}w_jh_j(x)$$\n","\n","In practice, most commonly used set of mixture weight is **uniform average\n","weighting**:\n","\n","$$f(x) = \\frac{1}{N}\\sum_{j=1}^{N}h_j(x)$$\n","\n","However, we can also solve a convex optimization problem to learn the mixture\n","weights that minimize the loss function $\\Phi$:\n","\n","$$F(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\Phi \\left(\\sum_{j=1}^{N}w_jh_j(x_i), y_i \\right)$$\n","\n","This is the first term in the AdaNet objective. The second term applies L1\n","regularization to the mixture weights:\n","\n","$$\\sum_{j=1}^{N} \\left(\\lambda r(h_j) + \\beta \\right) |w_j|$$\n","\n","When $\\lambda > 0$ this penalty serves to prevent the optimization from\n","assigning too much weight to more complex subnetworks according to the\n","complexity measure function $r$.\n","\n","## How AdaNet uses the objective\n","\n","This objective function serves two purposes:\n","\n","1.  To **learn to scale/transform the outputs of each subnetwork $h$** as part\n","    of the ensemble.\n","2.  To **select the best candidate subnetwork $h$** at each AdaNet iteration\n","    to include in the ensemble.\n","\n","Effectively, when learning mixture weights $w$, AdaNet solves a convex\n","combination of the outputs of the frozen subnetworks $h$. For $\\lambda >0$,\n","AdaNet penalizes more complex subnetworks with greater L1 regularization on\n","their mixture weight, and will be less likely to select more complex subnetworks\n","to add to the ensemble at each iteration.\n","\n","In this tutorial, you will observe the benefits of using AdaNet to learn the\n","ensemble's mixture weights and to perform candidate selection.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWSc6JsqAGjU"},"outputs":[],"source":["#@test {\"skip\": true}\n","# If you're running this in Colab, first install the adanet package:\n","!pip install adanet"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7777,"status":"ok","timestamp":1667465204355,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"t4ptB-vwWEGH","outputId":"e3833548-89f3-4447-8d0f-20c26b6b539b"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/adanet/core/tpu_estimator.py:33: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n","\n"]}],"source":["import functools\n","import os\n","import shutil\n","\n","import adanet\n","import tensorflow.compat.v1 as tf\n","\n","# The random seed to use.\n","RANDOM_SEED = 42\n","\n","LOG_DIR = '/tmp/models'"]},{"cell_type":"markdown","metadata":{"id":"ElcMSpyg_dYO"},"source":["## Boston Housing dataset\n","\n","In this example, we will solve a regression task known as the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) to predict the price of suburban houses in Boston, MA in the 1970s. There are 13 numerical features, the labels are in thousands of dollars, and there are only 506 examples.\n"]},{"cell_type":"markdown","metadata":{"id":"Mp5TTBOJ_ZTU"},"source":["## Download the data\n","Conveniently, the data is available via Keras:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1667465236163,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"Plx4LtD4_XFY","outputId":"a3a31e4c-71b2-48ba-a3a0-194d2ad38331"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n","57026/57026 [==============================] - 0s 0us/step\n","Model inputs: [  1.23247   0.        8.14      0.        0.538     6.142    91.7\n","   3.9769    4.      307.       21.      396.9      18.72   ] \n","\n","Model output (house price): $15200.0 \n"]}],"source":["(x_train, y_train), (x_test, y_test) = (\n","    tf.keras.datasets.boston_housing.load_data())\n","\n","# Preview the first example from the training data\n","print('Model inputs: %s \\n' % x_train[0])\n","print('Model output (house price): $%s ' % (y_train[0] * 1000))\n"]},{"cell_type":"markdown","metadata":{"id":"bUSb0JtOA5Xc"},"source":["## Supply the data in TensorFlow\n","\n","Our first task is to supply the data in TensorFlow. Using the\n","tf.estimator.Estimator convention, we will define a function that returns an\n","input_fn which returns feature and label Tensors.\n","\n","We will also use the tf.data.Dataset API to feed the data into our models.\n","\n","Also, as a preprocessing step, we will apply `tf.log1p` to log-scale the\n","features and labels for improved numerical stability during training. To recover\n","the model's predictions in the correct scale, you can apply `tf.math.expm1` to the\n","prediction."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1667465241342,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"H1U6DppVwTPM"},"outputs":[],"source":["FEATURES_KEY = \"x\"\n","\n","\n","def input_fn(partition, training, batch_size):\n","  \"\"\"Generate an input function for the Estimator.\"\"\"\n","\n","  def _input_fn():\n","\n","    if partition == \"train\":\n","      dataset = tf.data.Dataset.from_tensor_slices(({\n","          FEATURES_KEY: tf.log1p(x_train)\n","      }, tf.log1p(y_train)))\n","    else:\n","      dataset = tf.data.Dataset.from_tensor_slices(({\n","          FEATURES_KEY: tf.log1p(x_test)\n","      }, tf.log1p(y_test)))\n","\n","    # We call repeat after shuffling, rather than before, to prevent separate\n","    # epochs from blending together.\n","    if training:\n","      dataset = dataset.shuffle(10 * batch_size, seed=RANDOM_SEED).repeat()\n","\n","    dataset = dataset.batch(batch_size)\n","    iterator = dataset.make_one_shot_iterator()\n","    features, labels = iterator.get_next()\n","    return features, labels\n","\n","  return _input_fn"]},{"cell_type":"markdown","metadata":{"id":"XXy9pqlq_PQx"},"source":["## Define the subnetwork generator\n","\n","Let's define a subnetwork generator similar to the one in\n","[[Cortes et al., ICML 2017](https://arxiv.org/abs/1607.01097)] and in\n","`simple_dnn.py` which creates two candidate fully-connected neural networks at\n","each iteration with the same width, but one an additional hidden layer. To make\n","our generator *adaptive*, each subnetwork will have at least the same number\n","of hidden layers as the most recently added subnetwork to the\n","`previous_ensemble`.\n","\n","We define the complexity measure function $r$ to be $r(h) = \\sqrt{d(h)}$, where\n","$d$ is the number of hidden layers in the neural network $h$, to approximate the\n","Rademacher bounds from\n","[[Golowich et. al, 2017](https://arxiv.org/abs/1712.06541)]. So subnetworks\n","with more hidden layers, and therefore more capacity, will have more heavily\n","regularized mixture weights."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1667465257833,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"hZn9VeFUHvD-"},"outputs":[],"source":["_NUM_LAYERS_KEY = \"num_layers\"\n","\n","\n","class _SimpleDNNBuilder(adanet.subnetwork.Builder):\n","  \"\"\"Builds a DNN subnetwork for AdaNet.\"\"\"\n","\n","  def __init__(self, optimizer, layer_size, num_layers, learn_mixture_weights,\n","               seed):\n","    \"\"\"Initializes a `_DNNBuilder`.\n","\n","    Args:\n","      optimizer: An `Optimizer` instance for training both the subnetwork and\n","        the mixture weights.\n","      layer_size: The number of nodes to output at each hidden layer.\n","      num_layers: The number of hidden layers.\n","      learn_mixture_weights: Whether to solve a learning problem to find the\n","        best mixture weights, or use their default value according to the\n","        mixture weight type. When `False`, the subnetworks will return a no_op\n","        for the mixture weight train op.\n","      seed: A random seed.\n","\n","    Returns:\n","      An instance of `_SimpleDNNBuilder`.\n","    \"\"\"\n","\n","    self._optimizer = optimizer\n","    self._layer_size = layer_size\n","    self._num_layers = num_layers\n","    self._learn_mixture_weights = learn_mixture_weights\n","    self._seed = seed\n","\n","  def build_subnetwork(self,\n","                       features,\n","                       logits_dimension,\n","                       training,\n","                       iteration_step,\n","                       summary,\n","                       previous_ensemble=None):\n","    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n","\n","    input_layer = tf.to_float(features[FEATURES_KEY])\n","    kernel_initializer = tf.glorot_uniform_initializer(seed=self._seed)\n","    last_layer = input_layer\n","    for _ in range(self._num_layers):\n","      last_layer = tf.layers.dense(\n","          last_layer,\n","          units=self._layer_size,\n","          activation=tf.nn.relu,\n","          kernel_initializer=kernel_initializer)\n","    logits = tf.layers.dense(\n","        last_layer,\n","        units=logits_dimension,\n","        kernel_initializer=kernel_initializer)\n","\n","    shared = {_NUM_LAYERS_KEY: self._num_layers}\n","    return adanet.Subnetwork(\n","        last_layer=last_layer,\n","        logits=logits,\n","        complexity=self._measure_complexity(),\n","        shared=shared)\n","\n","  def _measure_complexity(self):\n","    \"\"\"Approximates Rademacher complexity as the square-root of the depth.\"\"\"\n","    return tf.sqrt(tf.to_float(self._num_layers))\n","\n","  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n","                                iteration_step, summary, previous_ensemble):\n","    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n","    return self._optimizer.minimize(loss=loss, var_list=var_list)\n","\n","  @property\n","  def name(self):\n","    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n","\n","    if self._num_layers == 0:\n","      # A DNN with no hidden layers is a linear model.\n","      return \"linear\"\n","    return \"{}_layer_dnn\".format(self._num_layers)\n","\n","\n","class SimpleDNNGenerator(adanet.subnetwork.Generator):\n","  \"\"\"Generates a two DNN subnetworks at each iteration.\n","\n","  The first DNN has an identical shape to the most recently added subnetwork\n","  in `previous_ensemble`. The second has the same shape plus one more dense\n","  layer on top. This is similar to the adaptive network presented in Figure 2 of\n","  [Cortes et al. ICML 2017](https://arxiv.org/abs/1607.01097), without the\n","  connections to hidden layers of networks from previous iterations.\n","  \"\"\"\n","\n","  def __init__(self,\n","               optimizer,\n","               layer_size=64,\n","               learn_mixture_weights=False,\n","               seed=None):\n","    \"\"\"Initializes a DNN `Generator`.\n","\n","    Args:\n","      optimizer: An `Optimizer` instance for training both the subnetwork and\n","        the mixture weights.\n","      layer_size: Number of nodes in each hidden layer of the subnetwork\n","        candidates. Note that this parameter is ignored in a DNN with no hidden\n","        layers.\n","      learn_mixture_weights: Whether to solve a learning problem to find the\n","        best mixture weights, or use their default value according to the\n","        mixture weight type. When `False`, the subnetworks will return a no_op\n","        for the mixture weight train op.\n","      seed: A random seed.\n","\n","    Returns:\n","      An instance of `Generator`.\n","    \"\"\"\n","\n","    self._seed = seed\n","    self._dnn_builder_fn = functools.partial(\n","        _SimpleDNNBuilder,\n","        optimizer=optimizer,\n","        layer_size=layer_size,\n","        learn_mixture_weights=learn_mixture_weights)\n","\n","  def generate_candidates(self, previous_ensemble, iteration_number,\n","                          previous_ensemble_reports, all_reports):\n","    \"\"\"See `adanet.subnetwork.Generator`.\"\"\"\n","\n","    num_layers = 0\n","    seed = self._seed\n","    if previous_ensemble:\n","      num_layers = previous_ensemble.subnetworks[-1].shared[_NUM_LAYERS_KEY]\n","    if seed is not None:\n","      seed += iteration_number\n","    return [\n","        self._dnn_builder_fn(num_layers=num_layers, seed=seed),\n","        self._dnn_builder_fn(num_layers=num_layers + 1, seed=seed),\n","    ]"]},{"cell_type":"markdown","metadata":{"id":"fhu2zpf9faIB"},"source":["## Launch TensorBoard\n","\n","Let's run [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) to visualize model training over time. We'll use [ngrok](https://ngrok.com/) to tunnel traffic to localhost.\n","\n","*The instructions for setting up Tensorboard were obtained from https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/*\n","\n","Run the next cells and follow the link to see the TensorBoard in a new tab."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3245,"status":"ok","timestamp":1667465281193,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"MLo_xWQMe686","outputId":"9a8d41a6-2992-447d-e28b-a3b829eb4350"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-11-03 08:48:41--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.19M  5.62MB/s    in 2.3s    \n","\n","2022-11-03 08:48:44 (5.62 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n","Follow this link to open TensorBoard in a new tab.\n","Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","IndexError: list index out of range\n"]}],"source":["#@test {\"skip\": true}\n","\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","# Install ngrok binary.\n","! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip\n","\n","# Delete old logs dir.\n","shutil.rmtree(LOG_DIR, ignore_errors=True)\n","\n","print(\"Follow this link to open TensorBoard in a new tab.\")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vJRH-v5NhnY9"},"source":["## Train and evaluate\n","\n","Next we create an `adanet.Estimator` using the `SimpleDNNGenerator` we just defined.\n","\n","In this section we will show the effects of two hyperparamters: **learning mixture weights** and **complexity regularization**.\n","\n","On the righthand side you will be able to play with the hyperparameters of this model. Until you reach the end of this section, we ask that you not change them. \n","\n","At first we will not learn the mixture weights, using their default initial value. Here they will be scalars initialized to $1/N$ where $N$ is the number of subnetworks in the ensemble, effectively creating a **uniform average ensemble**."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1667465293156,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"8TJHr1k-zYpN"},"outputs":[],"source":["#@title AdaNet parameters\n","LEARNING_RATE = 0.001  #@param {type:\"number\"}\n","TRAIN_STEPS = 60000  #@param {type:\"integer\"}\n","BATCH_SIZE = 32  #@param {type:\"integer\"}\n","\n","LEARN_MIXTURE_WEIGHTS = False  #@param {type:\"boolean\"}\n","ADANET_LAMBDA = 0  #@param {type:\"number\"}\n","ADANET_ITERATIONS = 3  #@param {type:\"integer\"}\n","\n","\n","def train_and_evaluate(experiment_name, learn_mixture_weights=LEARN_MIXTURE_WEIGHTS,\n","                       adanet_lambda=ADANET_LAMBDA):\n","  \"\"\"Trains an `adanet.Estimator` to predict housing prices.\"\"\"\n","\n","  model_dir = os.path.join(LOG_DIR, experiment_name)\n","\n","  ensembler_optimizer = None\n","  if learn_mixture_weights:\n","    ensembler_optimizer = tf.train.RMSPropOptimizer(\n","        learning_rate=LEARNING_RATE)\n","\n","  estimator = adanet.Estimator(\n","      # Since we are predicting housing prices, we'll use a regression\n","      # head that optimizes for MSE.\n","      head=tf.estimator.RegressionHead(),\n","\n","      # Define the generator, which defines our search space of subnetworks\n","      # to train as candidates to add to the final AdaNet model.\n","      subnetwork_generator=SimpleDNNGenerator(\n","          optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n","          learn_mixture_weights=learn_mixture_weights,\n","          seed=RANDOM_SEED),\n","\n","      # The number of train steps per iteration.\n","      max_iteration_steps=TRAIN_STEPS // ADANET_ITERATIONS,\n","\n","      # The evaluator will evaluate the model on the full training set to\n","      # compute the overall AdaNet loss (train loss + complexity\n","      # regularization) to select the best candidate to include in the\n","      # final AdaNet model.\n","      evaluator=adanet.Evaluator(\n","          input_fn=input_fn(\"train\", training=False, batch_size=BATCH_SIZE)),\n","\n","      ensemblers=[\n","          adanet.ensemble.ComplexityRegularizedEnsembler(\n","              optimizer=ensembler_optimizer,\n","              # Lambda is a the strength of complexity regularization. A larger\n","              # value will penalize more complex subnetworks.\n","              adanet_lambda=adanet_lambda),\n","      ],      \n","\n","      # Configuration for Estimators.\n","      config=tf.estimator.RunConfig(\n","          save_summary_steps=5000,\n","          save_checkpoints_steps=5000,\n","          tf_random_seed=RANDOM_SEED,\n","          model_dir=model_dir))\n","\n","  # Train and evaluate using using the tf.estimator tooling.\n","  train_spec = tf.estimator.TrainSpec(\n","      input_fn=input_fn(\"train\", training=True, batch_size=BATCH_SIZE),\n","      max_steps=TRAIN_STEPS)\n","  eval_spec = tf.estimator.EvalSpec(\n","      input_fn=input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n","      steps=None,\n","      start_delay_secs=1,\n","      throttle_secs=30,\n","  )\n","  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","  return estimator.evaluate(\n","      input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n","      steps=None)\n","\n","def ensemble_architecture(result):\n","  \"\"\"Extracts the ensemble architecture from evaluation results.\"\"\"\n","\n","  architecture = result[\"architecture/adanet/ensembles\"]\n","  # The architecture is a serialized Summary proto for TensorBoard.\n","  summary_proto = tf.summary.Summary.FromString(architecture)\n","  return summary_proto.value[0].tensor.string_val[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218070,"status":"ok","timestamp":1667465526851,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"},"user_tz":-180},"id":"U27fd6Kwhk3r","outputId":"5cc60e98-5c51-4c68-d283-a7f1963d57cf"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f36635950>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From <ipython-input-4-48abf98e4055>:24: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/adanet/core/estimator.py:1480: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2dd84710>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2cc14f80>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2f0704d0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1175: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1068: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2edf13b0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2cc0aef0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f3679fdd0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2fa3d050>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2d079ef0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2dd6f0e0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2e975050>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f26697d40>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f23f4e710>) includes params argument, but params are not passed to Estimator.\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 0.038238864\n","Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"]}],"source":["results = train_and_evaluate(\"uniform_average_ensemble_baseline\")\n","print(\"Loss:\", results[\"average_loss\"])\n","print(\"Architecture:\", ensemble_architecture(results))"]},{"cell_type":"markdown","metadata":{"id":"A0vBGKeTj0Kf"},"source":["These hyperparameters preduce a model that achieves **0.0351** MSE on the test\n","set (exact MSE will vary depending on the hardware you're using to train the model). Notice that the ensemble is composed of 3 subnetworks, each one a hidden\n","layer deeper than the previous. The most complex subnetwork is made of 3 hidden\n","layers.\n","\n","Since `SimpleDNNGenerator` produces subnetworks of varying complexity, and our\n","model gives each one an equal weight, AdaNet selected the subnetwork that most\n","lowered the ensemble's training loss at each iteration, likely the one with the\n","most hidden layers, since it has the most capacity, and we aren't penalizing\n","more complex subnetworks (yet).\n","\n","Next, instead of assigning equal weight to each subnetwork, let's learn the\n","mixture weights as a convex optimization problem using SGD:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXfJ6cb_KO35","outputId":"b1b5685a-f476-4232-ab44-e75486b210df","executionInfo":{"status":"ok","timestamp":1667465893194,"user_tz":-180,"elapsed":308427,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2e462f80>) includes params argument, but params are not passed to Estimator.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f28923830>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2874c0e0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2cc2fb90>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2cc2fb90>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2ebbd9e0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f3392fa70>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2f0cf7a0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f241df950>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2f6d1b90>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2eb9bef0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f2e72d950>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f26336830>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f5f287c69e0>) includes params argument, but params are not passed to Estimator.\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 0.038964078\n","Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"]}],"source":["#@test {\"skip\": true}\n","results = train_and_evaluate(\"learn_mixture_weights\", learn_mixture_weights=True)\n","print(\"Loss:\", results[\"average_loss\"])\n","print(\"Architecture:\", ensemble_architecture(results))"]},{"cell_type":"markdown","metadata":{"id":"BTUdJ_FWl84O"},"source":["Learning the mixture weights produces a model with **0.0343** MSE, a bit better\n","than the uniform average model, which the `adanet.Estimator` always compute as a\n","baseline. The mixture weights were learned without regularization, so they\n","risk overfitting the training set.\n","\n","Observe that AdaNet learned the same ensemble composition as the previous run.\n","Without complexity regularization, AdaNet will favor more complex subnetworks,\n","which may have worse generalization despite improving the empirical error.\n","\n","Finally, let's apply some **complexity regularization** by using $\\lambda > 0$.\n","Since this will penalize more complex subnetworks, AdaNet will select the\n","candidate subnetwork that most improves the objective for its marginal\n","complexity:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g_C-dT7KaTy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663670017366,"user_tz":-180,"elapsed":251964,"user":{"displayName":"Павел Сахнюк","userId":"12726213032567316501"}},"outputId":"e7cb8581-f292-4126-8de3-4397a9922fc4"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f530feef0>) includes params argument, but params are not passed to Estimator.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5e202200>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53eeb950>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5307b0e0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5b07c320>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53f4aa70>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5f112290>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f53857170>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5c8164d0>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f626b4d40>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5dfa5b00>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5d38bb00>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5792cc20>) includes params argument, but params are not passed to Estimator.\n","WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n","WARNING:tensorflow:Estimator's model_fn (<function Estimator._create_model_fn.<locals>._adanet_model_fn at 0x7f9f5404e320>) includes params argument, but params are not passed to Estimator.\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 0.038760666\n","Architecture: b'| 1_layer_dnn | 2_layer_dnn | 3_layer_dnn |'\n"]}],"source":["#@test {\"skip\": true}\n","results = train_and_evaluate(\"learn_mixture_weights_with_complexity_regularization\", learn_mixture_weights=True, adanet_lambda=.001)\n","print(\"Loss:\", results[\"average_loss\"])\n","print(\"Architecture:\", ensemble_architecture(results))"]},{"cell_type":"markdown","metadata":{"id":"s5RuM582oxo_"},"source":["Learning the mixture weights with $\\lambda > 0$ produces a model with **0.0340**\n","MSE.\n","\n","Inspecting the ensemble architecture demonstrates the effects of complexity\n","regularization on candidate selection. The selected subnetworks are relatively\n","less complex: unlike in previous runs, the deepest subnetwork has only 2 hidden layers."]},{"cell_type":"markdown","metadata":{"id":"ajRhOvHGQu6w"},"source":["## Conclusion\n","\n","In this tutorial, you were able to explore training an AdaNet model's mixture\n","weights with $\\lambda \\ge 0$. You were also able to compare against building an\n","ensemble formed by always choosing the best candidate subnetwork at each\n","iteration based on it's ability to improve the ensemble's loss on the training\n","set, and averaging their results.\n","\n","Uniform average ensembles work unreasonably well in practice, yet learning the\n","mixture weights with the correct values of $\\lambda$ and $\\beta$ should always\n","produce a better model when candidates have varying complexity. However, this\n","does require some additional hyperparameter tuning, so practically you can train\n","an AdaNet with the default mixture weights and $\\lambda=0$ first, and once you\n","have confirmed that the subnetworks are training correctly, you can tune the\n","mixture weight hyperparameters.\n","\n","While this example explored a regression task, these observations apply to using\n","AdaNet on other tasks like binary-classification and multi-class classification."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}